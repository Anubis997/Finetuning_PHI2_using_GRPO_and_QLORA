{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WdT1jTUyPhC6"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-n4xKqAVT1a"
      },
      "outputs": [],
      "source": [
        "#!pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HEnkQZUFYgkx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft trl accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_le9xspiEU7X"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Y7PR5EbIE17a"
      },
      "outputs": [],
      "source": [
        "!pip install vllm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JDZgGdHySve"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Make sure to download the required resources\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5ikge_iPnJL"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import torch\n",
        "import os\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "import bitsandbytes as bnb\n",
        "import math\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from unsloth import FastLanguageModel, PatchFastRL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xhS-U3Oei4Ni"
      },
      "outputs": [],
      "source": [
        "# Load train and validation datasets directly\n",
        "train_dataset = load_dataset(\"trl-lib/tldr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yM8LyHMIHDW2"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Combine train + validation + test\n",
        "full_dataset = concatenate_datasets([\n",
        "    train_dataset[\"train\"],\n",
        "    train_dataset[\"validation\"],\n",
        "    train_dataset[\"test\"],\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H3cM6I5qjEbs"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Optional: Patch RL logic (used only in RLHF-type training) ===\n",
        "PatchFastRL(\"GRPO\", FastLanguageModel)\n",
        "\n",
        "# === Config ===\n",
        "model_name = \"microsoft/phi-2\"\n",
        "max_seq_length = 512\n",
        "lora_rank = 8\n",
        "use_4bit = True\n",
        "\n",
        "# === Load model and tokenizer via Unsloth (replaces AutoModelForCausalLM + BitsAndBytesConfig) ===\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = use_4bit,\n",
        "    fast_inference = True,         # Use vLLM-style speedups\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.5,  # Adjust based on VRAM\n",
        ")\n",
        "\n",
        "# === Unsloth handles tokenizer defaults â€” but you can explicitly set these too: ===\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# === Apply LoRA ===\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    lora_alpha = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xY9pN_virqb"
      },
      "outputs": [],
      "source": [
        "# 3. Optimized GRPO config\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate=5e-4,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    fp16=True,\n",
        "    per_device_train_batch_size=6,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_generations=6,  # reduced from 6\n",
        "    max_prompt_length=256,\n",
        "    max_completion_length=128,\n",
        "    eval_steps=5,\n",
        "    max_steps=300,\n",
        "    save_steps=20,\n",
        "    max_grad_norm = 0.1,\n",
        "    logging_steps=5,\n",
        "    report_to=\"none\",\n",
        "    output_dir=\"./phi2-grpo-results\",\n",
        "    remove_unused_columns=False,\n",
        "    label_names=[]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDwNVCTAzNAg"
      },
      "outputs": [],
      "source": [
        "def reward_bleu(prompts, completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function that scores based on BLEU score for the completion\n",
        "    Args:\n",
        "        prompts: list of input prompts (not used in BLEU directly)\n",
        "        completions: list of generated completions\n",
        "        **kwargs: additional arguments passed by the trainer\n",
        "    Returns:\n",
        "        list of reward scores based on BLEU\n",
        "    \"\"\"\n",
        "    # This is just an example target. You'd want to adjust based on your task.\n",
        "    reference = kwargs.get(\"reference\", [\"This is a placeholder reference\"])\n",
        "\n",
        "    rewards = []\n",
        "    smoothing_function = SmoothingFunction().method4  # Use smoothing to avoid zero BLEU score\n",
        "\n",
        "    for completion in completions:\n",
        "        # Tokenize the generated completion\n",
        "        generated_tokens = nltk.word_tokenize(completion.lower())\n",
        "\n",
        "        # Compute BLEU score\n",
        "        bleu_score = sentence_bleu([reference], generated_tokens, smoothing_function=smoothing_function)\n",
        "        rewards.append(bleu_score)\n",
        "\n",
        "    return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMgR9zqezrU0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rwR8LBc0Rjz"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "59G3Bhr9jq1l"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=reward_bleu,\n",
        "    args=training_args,\n",
        "    train_dataset=full_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model(\"./phi2-grpo-final\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkUCuMoPH2i2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Define the quantization config (make sure to have the `bnb_config` set up)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",  # Load Phi-2 model from Hugging Face # Use the appropriate quantization config (like QLoRA)\n",
        "    device_map=\"auto\"  # Automatically map model to the available device (e.g., GPU/CPU)\n",
        ")\n",
        "\n",
        "phi2_tokenizer =AutoTokenizer.from_pretrained(\"microsoft/phi-2\")  # Adjust the tokenizer name if needed\n",
        "\n",
        "# Load the GRPO fine-tuned model from the checkpoint folder\n",
        "grpo_model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/Checkpoint_300\")\n",
        "grpo_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Checkpoint_300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "qlm-LelOK3xe"
      },
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length=150, device='cuda'):\n",
        "    # Ensure model is on the correct device\n",
        "    model.to(device)\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "    # Move input tensors to the same device as the model\n",
        "    input_ids = inputs.input_ids.to(device)\n",
        "    attention_mask = inputs.get('attention_mask', None)\n",
        "    if attention_mask is not None:\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "    # Generate the output\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        temperature=0.9\n",
        "    )\n",
        "\n",
        "    # Decode the output tokens back to text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Ensure both models are on the same device (e.g., 'cuda' or 'cpu')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Get responses from both models, making sure they're on the same device\n",
        "phi2_responses = [generate_response(model, phi2_tokenizer, prompt, device=device) for prompt in prompts]\n",
        "grpo_responses = [generate_response(grpo_model, grpo_tokenizer, prompt, device=device) for prompt in prompts]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azDY38ESLIVp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Display the responses\n",
        "for prompt, phi2_response, grpo_response in zip(prompts, phi2_responses, grpo_responses):\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Phi-2 Response: {phi2_response}\")\n",
        "    print(f\"GRPO + QLoRA Response: {grpo_response}\")\n",
        "    print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ull3WjfALvjv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}